{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FineTuning.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cosminnedescu/ProjectMLDL/blob/main/classifiers/iCaRL-v1_3_0-SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PF5bRIxnU-PW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8067e43-1cae-4c90-b776-f6b42ba706a5"
      },
      "source": [
        "# Avoid K80\n",
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Jul  3 09:05:50 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJimWkPdQu6y"
      },
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets, models\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib \n",
        "import matplotlib.pyplot as plt\n",
        "from copy import copy\n",
        "from copy import deepcopy"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BZw-bZJ16ay"
      },
      "source": [
        "#### Cloning the Git repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjntw1jZQ7Lg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7272c54c-3a2b-4ce1-b6fb-198d8bcf9bf6"
      },
      "source": [
        "!rm -rf ProjectMLDL\n",
        "if not os.path.isdir('/content/ProjectMLDL'):\n",
        "  !git clone https://github.com/cosminnedescu/ProjectMLDL.git\n",
        "  %cd /content/ProjectMLDL\n",
        "  !rm -rf LICENSE README.md"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ProjectMLDL'...\n",
            "remote: Enumerating objects: 1838, done.\u001b[K\n",
            "remote: Counting objects: 100% (324/324), done.\u001b[K\n",
            "remote: Compressing objects: 100% (251/251), done.\u001b[K\n",
            "remote: Total 1838 (delta 240), reused 73 (delta 73), pack-reused 1514\u001b[K\n",
            "Receiving objects: 100% (1838/1838), 69.81 MiB | 30.02 MiB/s, done.\n",
            "Resolving deltas: 100% (986/986), done.\n",
            "/content/ProjectMLDL\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvqmxRxo2lUW"
      },
      "source": [
        "from data.cifar100 import CIFAR100\n",
        "from model.resnet32 import resnet32\n",
        "import data.utils\n",
        "from model.icarl import iCaRL"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwomFTtQo1x-"
      },
      "source": [
        "# True mean and std of Cifar100 dataset (src=\"https://gist.github.com/weiaicunzai/e623931921efefd4c331622c344d8151\")\n",
        "mean = [0.5071, 0.4867, 0.4408]\n",
        "std = [0.2675, 0.2565, 0.2761]\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "test_transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), \n",
        "     transforms.Normalize(mean, std),\n",
        "     ])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRoS-R_sRhHT"
      },
      "source": [
        "## Incremental Classifier and Representation Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ocVW0wM2Iw2"
      },
      "source": [
        "### Defining hyperparameters according to iCarl paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmEo_9lnwyBk"
      },
      "source": [
        "# Settings\n",
        "DEVICE = 'cuda'\n",
        "NUM_CLASSES = 100         # Total number of classes\n",
        "VAL_SIZE = 0.2            # Proportion of validation set with respect to training set (between 0 and 1)\n",
        "\n",
        "# Training\n",
        "BATCH_SIZE = 128          # Batch size\n",
        "LR = 2                    # Initial learning rate\n",
        "                       \n",
        "MOMENTUM = 0.9            # Momentum for stochastic gradient descent (SGD)\n",
        "WEIGHT_DECAY = 1e-5       # Weight decay from iCaRL\n",
        "\n",
        "RANDOM_SEED = [42]  # Random seeds defining the runs of every method\n",
        "                          # Note: this should be at least 3 to have a fair benchmark\n",
        "\n",
        "NUM_EPOCHS = 7           # Total number of training epochs\n",
        "MILESTONES = [49, 63]     # Step down policy from iCaRL (MultiStepLR)\n",
        "                          # Decrease the learning rate by gamma at each milestone\n",
        "GAMMA = 0.2               # Gamma factor from iCaRL (1/5)\n",
        "\n",
        "HERDING = False           # True to perform prioritized selection, False to perform random selection\n",
        "CLASSIFY = True           # True to use mean-of-exemplar classifier, False to use network's output directly for classification"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fs288ExTNcpa"
      },
      "source": [
        "from data.exemplar import Exemplar\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection  import ParameterGrid\n",
        "from torch.backends import cudnn"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Plgm6P1MEUw"
      },
      "source": [
        "class SVM_Classifier(iCaRL):\n",
        "  \n",
        "  def __init__(self, device, net, LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, train_dl, validation_dl, test_dl, BATCH_SIZE, train_subset, train_transform, test_transform, params):\n",
        "    super().__init__(device, net, LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, train_dl, validation_dl, test_dl, BATCH_SIZE, train_subset, train_transform, test_transform)\n",
        "    #self.validation_set = val_set\n",
        "    #self.test_set = test_set\n",
        "    self.PARAMS = params\n",
        "\n",
        "  def separate_data(self, data):\n",
        "    \"\"\"\n",
        "    X = np.zeros((len(data), 3, 32, 32))\n",
        "    y = np.zeros(len(data), dtype=int)\n",
        "    \n",
        "    dataloader = DataLoader(data, batch_size=1)\n",
        "    for i, (_, images, labels) in enumerate(dataloader):\n",
        "      X[i] = images[0].numpy()\n",
        "      y[i] = labels.numpy()[0]\n",
        "    X_features = self.features_extractor(torch.tensor(X, dtype=torch.float))\n",
        "    for i in range(X_features.size(0)):\n",
        "      X_features[i] = X_features[i]/X_features[i].norm()\n",
        "    X_features = X_features.to('cpu').numpy()\n",
        "    return X_features, y\n",
        "    \"\"\"\n",
        "    all_features = torch.tensor([])\n",
        "    all_features = all_features.type(torch.LongTensor)\n",
        "    all_targets = torch.tensor([])\n",
        "    all_targets = all_targets.type(torch.LongTensor)\n",
        "    for _, images, labels in data:\n",
        "      images = images.to(self.DEVICE)\n",
        "      labels = labels.to(self.DEVICE)\n",
        "      \n",
        "      all_targets = torch.cat((all_targets.to(self.DEVICE), labels.to(self.DEVICE)), dim=0)\n",
        "      feature_map = self.features_extractor(images)\n",
        "      for i in range(feature_map.size(0)):\n",
        "        feature_map[i] = feature_map[i] / feature_map[i].norm()\n",
        "      feature_map = feature_map.to(self.DEVICE)\n",
        "      all_features = torch.cat((all_features.to(self.DEVICE), feature_map.to(self.DEVICE)), dim=0)\n",
        "    return all_features.detach().cpu(), all_targets.detach().cpu()\n",
        "    \n",
        "    \n",
        "  def fit_train_data(self, classes_group_idx):\n",
        "    \n",
        "    #exemplars = Exemplar(self.exemplar_set, self.train_transform)\n",
        "    #train_set.dataset.set_transform_status(True)\n",
        "    #ex_train_set = ConcatDataset([exemplars, train_set])\n",
        "    X_train, y_train = self.separate_data(self.train_dl[classes_group_idx])\n",
        "    #val_set.dataset.set_transform_status(True)\n",
        "    X_test, y_test = self.separate_data(self.validation_dl[classes_group_idx])\n",
        "    \n",
        "    self.clf = SVC()   \n",
        "    best_clf = None\n",
        "    best_grid = None\n",
        "    best_score = 0\n",
        "    \n",
        "    for grid in ParameterGrid(self.PARAMS):\n",
        "        self.clf.set_params(**grid)\n",
        "        self.clf.fit(X_train, y_train)\n",
        "        y_pred = self.clf.predict(X_test)\n",
        "        score = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        if score > best_score:\n",
        "            best_clf = deepcopy(self.clf)\n",
        "            best_score = score\n",
        "            best_grid = grid\n",
        "    else:\n",
        "      self.clf = best_clf\n",
        "\n",
        "    print(f\"Best classifier: {best_grid} with score {best_score}\")\n",
        "  \n",
        "  def predict_test_data(self, classes_group_idx):\n",
        "    X_test, y_test = self.separate_data(self.test_dl[classes_group_idx])\n",
        "    y_pred = self.clf.predict(X_test)\n",
        "    return y_test, y_pred\n",
        "  \n",
        "  def test_classify(self, classes_group_idx, train_set):\n",
        "    self.best_net.train(False)\n",
        "    if self.best_net is not None: self.best_net.train(False)\n",
        "    if self.old_net is not None: self.old_net.train(False)\n",
        "\n",
        "    all_preds = torch.tensor([])\n",
        "    all_preds = all_preds.type(torch.LongTensor)\n",
        "    all_targets = torch.tensor([])\n",
        "    all_targets = all_targets.type(torch.LongTensor)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      self.fit_train_data(classes_group_idx)\n",
        "      labels, preds = self.predict_test_data(classes_group_idx)\n",
        "      accuracy = accuracy_score(labels, preds)\n",
        "\n",
        "      labels = torch.tensor(labels)\n",
        "      preds = torch.tensor(preds)\n",
        "      all_targets = torch.cat((all_targets.to(self.DEVICE), labels.to(self.DEVICE)), dim=0)\n",
        "      all_preds = torch.cat((all_preds.to(self.DEVICE), preds.to(self.DEVICE)), dim=0) \n",
        "\n",
        "    return accuracy, all_targets, all_preds"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVh61C63FECy"
      },
      "source": [
        "def do_group_classes(run):\n",
        "\n",
        "  train_subset = [[] for i in range(10)]\n",
        "  val_subset = [[] for i in range(10)]\n",
        "  test_set = [[] for i in range(10)]\n",
        "  train_dataloader = [[] for i in range(10)]\n",
        "  val_dataloader = [[] for i in range(10)]\n",
        "  test_dataloader = [[] for i in range(10)]\n",
        "\n",
        "  for i in range(10):\n",
        "    train_data = CIFAR100(\"dataset\", \n",
        "                          train=True, \n",
        "                          transform=train_transform, \n",
        "                          download=(run+i==0),\n",
        "                          random_state=RANDOM_SEED[run])\n",
        "    test_data = CIFAR100(\"dataset\", \n",
        "                         train=False, \n",
        "                         transform=test_transform, \n",
        "                         download=False,\n",
        "                         random_state=RANDOM_SEED[run])\n",
        "    \n",
        "    train_data.set_index_map(train_data.splits[i])\n",
        "    test_data.set_index_map([test_data.splits[j] for j in range(0, i+1)])\n",
        "    \n",
        "    train_indices, val_indices = train_data.train_val_split(VAL_SIZE, RANDOM_SEED[run])\n",
        "    \n",
        "    train_subset[i] = copy(Subset(train_data, train_indices))\n",
        "    val_subset[i] = Subset(train_data, val_indices)\n",
        "    test_set[i] = copy(test_data)\n",
        "\n",
        "    tmp_dl = DataLoader(val_subset[i],\n",
        "                       batch_size=BATCH_SIZE,\n",
        "                       shuffle=True, \n",
        "                       num_workers=4,\n",
        "                       drop_last=True)\n",
        "    val_dataloader[i] = copy(tmp_dl)\n",
        "\n",
        "    tmp_dl = DataLoader(test_data,\n",
        "                       batch_size=BATCH_SIZE,\n",
        "                       shuffle=True, \n",
        "                       num_workers=4,\n",
        "                       drop_last=True)\n",
        "    test_dataloader[i] = copy(tmp_dl)\n",
        "\n",
        "  return train_dataloader, val_dataloader, test_dataloader, train_subset, val_subset, test_set"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20APZIgI1eBy"
      },
      "source": [
        "### Going on with the model\n",
        "This is the main iCaRL step.\n",
        "\n",
        "This step is run 3 times with different `RANDOM_SEED`.\n",
        "Here the model is instantiated, trained and tested.\n",
        "\n",
        "Results and some statistics are then stored in the variable `logs`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXYig-FtUlc_"
      },
      "source": [
        "params = {'C': [.1, 1, 10, 100]}"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJHKbipz0bMr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db30143a-ba80-4491-859f-c133b54bcfaf"
      },
      "source": [
        "logs = [[] for i in range(len(RANDOM_SEED))]\n",
        "best_net_tot_classes = [None for i in range(len(RANDOM_SEED))]\n",
        "\n",
        "for run in range(len(RANDOM_SEED)):\n",
        "  print(\"#################################\")\n",
        "  print(f\"Radom seed: {RANDOM_SEED[run]}\")\n",
        "  print(\"\")\n",
        "\n",
        "  # get data_subsets separated in incremental groups of 10 classes\n",
        "  train_dl, val_dl, test_dl, train_set, val_set, test_set = do_group_classes(run)\n",
        "\n",
        "  #create the resnet\n",
        "  net = resnet32()\n",
        "  \n",
        "  trainer = SVM_Classifier(DEVICE,\n",
        "                  net,\n",
        "                  LR,\n",
        "                  MOMENTUM,\n",
        "                  WEIGHT_DECAY,\n",
        "                  MILESTONES,\n",
        "                  GAMMA,\n",
        "                  train_dl,\n",
        "                  val_dl,\n",
        "                  test_dl,\n",
        "                  BATCH_SIZE,\n",
        "                  train_set,\n",
        "                  train_transform,\n",
        "                  test_transform,\n",
        "                  params)\n",
        "\n",
        "  #train and evaluate the model\n",
        "  logs[run] = trainer.train_model(NUM_EPOCHS, HERDING, CLASSIFY)\n",
        "\n",
        "  best_net_tot_classes[run] = deepcopy(trainer.best_net)\n",
        "\n",
        "  print(\"#################################\")\n",
        "  print(\"\")\n",
        "  print(\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#################################\n",
            "Radom seed: 42\n",
            "\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Length of exemplars set: 0\n",
            "Epoch 1/7 LR: [2]\n",
            "Validation accuracy on group 1/10: 0.10\n",
            "Best model updated\n",
            "\n",
            "Epoch 2/7 LR: [2]\n",
            "Validation accuracy on group 1/10: 0.10\n",
            "Best model updated\n",
            "\n",
            "Epoch 3/7 LR: [2]\n",
            "Validation accuracy on group 1/10: 0.09\n",
            "\n",
            "Epoch 4/7 LR: [2]\n",
            "Validation accuracy on group 1/10: 0.10\n",
            "Best model updated\n",
            "\n",
            "Epoch 5/7 LR: [2]\n",
            "Validation accuracy on group 1/10: 0.09\n",
            "\n",
            "Epoch 6/7 LR: [2]\n",
            "Validation accuracy on group 1/10: 0.11\n",
            "Best model updated\n",
            "\n",
            "Epoch 7/7 LR: [2]\n",
            "Validation accuracy on group 1/10: 0.11\n",
            "\n",
            "Group 1 Finished!\n",
            "Best accuracy found at epoch 6: 0.11\n",
            "Target number of exemplars: 200\n",
            "Randomly extracting exemplars from class 0 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 1 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 2 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 3 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 4 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 5 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 6 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 7 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 8 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 9 of current split... Extracted 200 exemplars.\n",
            "Best classifier: {'C': 100} with score 0.11160714285714286\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Testing classes seen so far, accuracy: 0.10\n",
            "\n",
            "=============================================\n",
            "\n",
            "Length of exemplars set: 2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/7 LR: [2]\n",
            "Validation accuracy on group 2/10: 0.00\n",
            "\n",
            "Epoch 2/7 LR: [2]\n",
            "Validation accuracy on group 2/10: 0.00\n",
            "\n",
            "Epoch 3/7 LR: [2]\n",
            "Validation accuracy on group 2/10: 0.00\n",
            "\n",
            "Epoch 4/7 LR: [2]\n",
            "Validation accuracy on group 2/10: 0.00\n",
            "\n",
            "Epoch 5/7 LR: [2]\n",
            "Validation accuracy on group 2/10: 0.06\n",
            "Best model updated\n",
            "\n",
            "Epoch 6/7 LR: [2]\n",
            "Validation accuracy on group 2/10: 0.00\n",
            "\n",
            "Epoch 7/7 LR: [2]\n",
            "Validation accuracy on group 2/10: 0.00\n",
            "\n",
            "Group 2 Finished!\n",
            "Best accuracy found at epoch 5: 0.06\n",
            "Target number of exemplars: 100\n",
            "Randomly extracting exemplars from class 0 of current split... Extracted 100 exemplars.\n",
            "Randomly extracting exemplars from class 1 of current split... Extracted 100 exemplars.\n",
            "Randomly extracting exemplars from class 2 of current split... Extracted 100 exemplars.\n",
            "Randomly extracting exemplars from class 3 of current split... Extracted 100 exemplars.\n",
            "Randomly extracting exemplars from class 4 of current split... Extracted 100 exemplars.\n",
            "Randomly extracting exemplars from class 5 of current split... Extracted 100 exemplars.\n",
            "Randomly extracting exemplars from class 6 of current split... Extracted 100 exemplars.\n",
            "Randomly extracting exemplars from class 7 of current split... Extracted 100 exemplars.\n",
            "Randomly extracting exemplars from class 8 of current split... Extracted 100 exemplars.\n",
            "Randomly extracting exemplars from class 9 of current split... Extracted 100 exemplars.\n",
            "Best classifier: {'C': 100} with score 0.12611607142857142\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Testing classes seen so far, accuracy: 0.05\n",
            "\n",
            "=============================================\n",
            "\n",
            "Length of exemplars set: 2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/7 LR: [2]\n",
            "Validation accuracy on group 3/10: 0.01\n",
            "Best model updated\n",
            "\n",
            "Epoch 2/7 LR: [2]\n",
            "Validation accuracy on group 3/10: 0.00\n",
            "\n",
            "Epoch 3/7 LR: [2]\n",
            "Validation accuracy on group 3/10: 0.01\n",
            "Best model updated\n",
            "\n",
            "Epoch 4/7 LR: [2]\n",
            "Validation accuracy on group 3/10: 0.01\n",
            "Best model updated\n",
            "\n",
            "Epoch 5/7 LR: [2]\n",
            "Validation accuracy on group 3/10: 0.02\n",
            "Best model updated\n",
            "\n",
            "Epoch 6/7 LR: [2]\n",
            "Validation accuracy on group 3/10: 0.04\n",
            "Best model updated\n",
            "\n",
            "Epoch 7/7 LR: [2]\n",
            "Validation accuracy on group 3/10: 0.04\n",
            "Best model updated\n",
            "\n",
            "Group 3 Finished!\n",
            "Best accuracy found at epoch 7: 0.04\n",
            "Target number of exemplars: 66\n",
            "Randomly extracting exemplars from class 0 of current split... Extracted 66 exemplars.\n",
            "Randomly extracting exemplars from class 1 of current split... Extracted 66 exemplars.\n",
            "Randomly extracting exemplars from class 2 of current split... Extracted 66 exemplars.\n",
            "Randomly extracting exemplars from class 3 of current split... Extracted 66 exemplars.\n",
            "Randomly extracting exemplars from class 4 of current split... Extracted 66 exemplars.\n",
            "Randomly extracting exemplars from class 5 of current split... Extracted 66 exemplars.\n",
            "Randomly extracting exemplars from class 6 of current split... Extracted 66 exemplars.\n",
            "Randomly extracting exemplars from class 7 of current split... Extracted 66 exemplars.\n",
            "Randomly extracting exemplars from class 8 of current split... Extracted 66 exemplars.\n",
            "Randomly extracting exemplars from class 9 of current split... Extracted 66 exemplars.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIUvaKyhx0rC"
      },
      "source": [
        "### Store logs in more usable dtype"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-uV6X0yxzQc"
      },
      "source": [
        "train_loss = [[logs[run_i]['group_train_loss'][i] for i in range(10)] for run_i in range(len(RANDOM_SEED))]\n",
        "train_accuracy = [[logs[run_i]['group_train_accuracies'][i] for i in range(10)] for run_i in range(len(RANDOM_SEED))]\n",
        "val_loss = [[logs[run_i]['val_losses'][i] for i in range(10)] for run_i in range(len(RANDOM_SEED))]\n",
        "val_accuracy = [[logs[run_i]['val_accuracies'][i] for i in range(10)] for run_i in range(len(RANDOM_SEED))]\n",
        "test_accuracy = [[logs[run_i]['test_accuracies'][i] for i in range(10)] for run_i in range(len(RANDOM_SEED))]\n",
        "predictions = [logs[run_i]['predictions'].cpu().data.numpy().tolist() for run_i in range(len(RANDOM_SEED))]\n",
        "true_labels = [logs[run_i]['true_labels'].cpu().data.numpy().tolist() for run_i in range(len(RANDOM_SEED))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r_ybcCLmc2R"
      },
      "source": [
        "### Save the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIG3hIOkyFNs"
      },
      "source": [
        "#### Saving logs in JSON files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mF_GR4KAyNYs"
      },
      "source": [
        "import json\n",
        "\n",
        "with open('states/iCaRL_Classifier_Cosine_train_loss.json', 'w') as f:\n",
        "  json.dump(train_loss, f)\n",
        "f.close\n",
        "with open('states/iCaRL_Classifier_Cosine_train_accuracy.json', 'w') as f:\n",
        "  json.dump(train_accuracy, f)\n",
        "f.close\n",
        "with open('states/iCaRL_Classifier_Cosine_val_loss.json', 'w') as f:\n",
        "  json.dump(val_loss, f)\n",
        "f.close  \n",
        "with open('states/iCaRL_Classifier_Cosine_val_accuracy.json', 'w') as f:\n",
        "  json.dump(val_accuracy, f)\n",
        "f.close\n",
        "with open('states/iCaRL_Classifier_Cosine_test_accuracy.json', 'w') as f:\n",
        "  json.dump(test_accuracy, f)\n",
        "f.close\n",
        "with open('states/iCaRL_Classifier_Cosine_predictions.json', 'w') as f:\n",
        "  json.dump(predictions, f)\n",
        "f.close\n",
        "with open('states/iCaRL_Classifier_Cosine_true_labels.json', 'w') as f:\n",
        "  json.dump(true_labels, f)\n",
        "f.close"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoVU9_V4zXOL"
      },
      "source": [
        "#### Saving best resnet on 100 classes for each seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nukM57Euzgr9"
      },
      "source": [
        "for i in range(len(RANDOM_SEED)):\n",
        "  torch.save(best_net_tot_classes[i].state_dict(), \"states/iCaRL_Classifier_Cosine_bestnet_seed[{}]\".format(RANDOM_SEED[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmaB173LxHiT"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('states/iCaRL_Classifier_Cosine_train_loss.json')\n",
        "files.download('states/iCaRL_Classifier_Cosine_train_accuracy.json')\n",
        "files.download('states/iCaRL_Classifier_Cosine_val_loss.json')\n",
        "files.download('states/iCaRL_Classifier_Cosine_val_accuracy.json')\n",
        "files.download('states/iCaRL_Classifier_Cosine_test_accuracy.json')\n",
        "files.download('states/iCaRL_Classifier_Cosine_predictions.json')\n",
        "files.download('states/iCaRL_Classifier_Cosine_true_labels.json')\n",
        "files.download('states/iCaRL_Classifier_Cosine_bestnet_seed[42]')\n",
        "files.download('states/iCaRL_Classifier_Cosine_bestnet_seed[13]')\n",
        "files.download('states/iCaRL_Classifier_Cosine_bestnet_seed[10]')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NMLqiIS19IJ"
      },
      "source": [
        "### Print some graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6Mg2QiIBNRo"
      },
      "source": [
        "from data.utils_plot import plot_train_val, plot_test_accuracies, plot_confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2QzOlExmhKo"
      },
      "source": [
        "train_loss = np.array(train_loss)\n",
        "train_accuracy = np.array(train_accuracy)\n",
        "val_loss = np.array(val_loss)\n",
        "val_accuracy = np.array(val_accuracy)\n",
        "test_accuracy = np.array(test_accuracy)\n",
        "\n",
        "train_loss_stats = np.array([train_loss.mean(0), train_loss.std(0)]).transpose()\n",
        "train_accuracy_stats = np.array([train_accuracy.mean(0), train_accuracy.std(0)]).transpose()\n",
        "val_loss_stats = np.array([val_loss.mean(0), val_loss.std(0)]).transpose()\n",
        "val_accuracy_stats = np.array([val_accuracy.mean(0), val_accuracy.std(0)]).transpose()\n",
        "test_accuracy_stats = np.array([test_accuracy.mean(0), test_accuracy.std(0)]).transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rB0p1PlFxrc"
      },
      "source": [
        "#### Train validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xy7FS53Mmmg-"
      },
      "source": [
        "plot_train_val(train_loss_stats, val_loss_stats, loss = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb_RNTNKF18b"
      },
      "source": [
        "#### Train validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05IT5LFUBBBV"
      },
      "source": [
        "plot_train_val(train_accuracy_stats, val_accuracy_stats, loss = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llzylw_LF7e_"
      },
      "source": [
        "#### Test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bou4H0k9mteX"
      },
      "source": [
        "plot_test_accuracies(test_accuracy_stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gN6cighYm1nG"
      },
      "source": [
        "#### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPG6aNRjm5Se"
      },
      "source": [
        "for run in range(len(RANDOM_SEED)):\n",
        "  targets = np.array(true_labels[run])\n",
        "  preds = np.array(predictions[run])\n",
        "\n",
        "  plot_confusion_matrix(targets, preds, RANDOM_SEED[run], 'iCaRL_Cosine')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}