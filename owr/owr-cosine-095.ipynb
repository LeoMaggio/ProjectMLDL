{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FineTuning.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cosminnedescu/ProjectMLDL/blob/main/owr/owr-cosine-095.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PF5bRIxnU-PW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "041f178c-82ea-485e-e89a-e2dd4dbda4f7"
      },
      "source": [
        "# Avoid K80\n",
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jul 12 14:11:52 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJimWkPdQu6y"
      },
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets, models\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib \n",
        "import matplotlib.pyplot as plt\n",
        "from copy import copy\n",
        "from copy import deepcopy"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BZw-bZJ16ay"
      },
      "source": [
        "#### Cloning the Git repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjntw1jZQ7Lg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb1facf2-eb0a-4ffe-eabb-bad857dc1bfd"
      },
      "source": [
        "!rm -rf ProjectMLDL\n",
        "if not os.path.isdir('/content/ProjectMLDL'):\n",
        "  !git clone https://github.com/cosminnedescu/ProjectMLDL.git\n",
        "  %cd /content/ProjectMLDL\n",
        "  !rm -rf LICENSE README.md"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ProjectMLDL'...\n",
            "remote: Enumerating objects: 3116, done.\u001b[K\n",
            "remote: Counting objects: 100% (532/532), done.\u001b[K\n",
            "remote: Compressing objects: 100% (302/302), done.\u001b[K\n",
            "remote: Total 3116 (delta 298), reused 396 (delta 222), pack-reused 2584\u001b[K\n",
            "Receiving objects: 100% (3116/3116), 167.37 MiB | 37.22 MiB/s, done.\n",
            "Resolving deltas: 100% (1683/1683), done.\n",
            "Checking out files: 100% (334/334), done.\n",
            "/content/ProjectMLDL\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvqmxRxo2lUW"
      },
      "source": [
        "from data.cifar100 import CIFAR100\n",
        "from model.resnet32_modified import resnet32\n",
        "import data.utils\n",
        "from model.OWR import owrCosine"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwomFTtQo1x-"
      },
      "source": [
        "# True mean and std of Cifar100 dataset (src=\"https://gist.github.com/weiaicunzai/e623931921efefd4c331622c344d8151\")\n",
        "mean = [0.5071, 0.4867, 0.4408]\n",
        "std = [0.2675, 0.2565, 0.2761]\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "test_transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), \n",
        "     transforms.Normalize(mean, std),\n",
        "     ])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRoS-R_sRhHT"
      },
      "source": [
        "## Incremental Classifier and Representation Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ocVW0wM2Iw2"
      },
      "source": [
        "### Defining hyperparameters according to iCarl paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmEo_9lnwyBk"
      },
      "source": [
        "# Settings\n",
        "DEVICE = 'cuda'\n",
        "NUM_CLASSES = 100         # Total number of classes\n",
        "VAL_SIZE = 0.2            # Proportion of validation set with respect to training set (between 0 and 1)\n",
        "\n",
        "# Training\n",
        "BATCH_SIZE = 128          # Batch size\n",
        "LR = 0.1                    # Initial learning rate\n",
        "                       \n",
        "MOMENTUM = 0.9            # Momentum for stochastic gradient descent (SGD)\n",
        "WEIGHT_DECAY = 1e-5       # Weight decay from iCaRL\n",
        "\n",
        "RANDOM_SEED = [20,24,66]  # Random seeds defining the runs of every method\n",
        "                          # Note: this should be at least 3 to have a fair benchmark\n",
        "\n",
        "NUM_EPOCHS = 70           # Total number of training epochs\n",
        "MILESTONES = [43, 63]     # Step down policy from iCaRL (MultiStepLR)\n",
        "                          # Decrease the learning rate by gamma at each milestone\n",
        "GAMMA = 0.2               # Gamma factor from iCaRL (1/5)\n",
        "\n",
        "HERDING = False           # True to perform prioritized selection, False to perform random selection\n",
        "CLASSIFY = True           # True to use mean-of-exemplar classifier, False to use network's output directly for classification\n",
        "\n",
        "REJECTION_THRESHOLD = 0.95\n",
        "TESTING_MODE = 'harmonic' #open, closed, harmonic"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVh61C63FECy"
      },
      "source": [
        "def do_group_classes(run):\n",
        "\n",
        "  train_subset = [[] for i in range(10)]\n",
        "  train_dataloader = [[] for i in range(10)]\n",
        "  val_dataloader = [[] for i in range(10)]\n",
        "  test_dataloader = [[] for i in range(10)]\n",
        "\n",
        "  for i in range(10):\n",
        "    train_data = CIFAR100(\"dataset\", \n",
        "                          train=True, \n",
        "                          transform=train_transform, \n",
        "                          download=(run+i==0),\n",
        "                          random_state=RANDOM_SEED[run])\n",
        "    test_data = CIFAR100(\"dataset\", \n",
        "                         train=False, \n",
        "                         transform=test_transform, \n",
        "                         download=False,\n",
        "                         random_state=RANDOM_SEED[run])\n",
        "    \n",
        "    train_data.set_index_map(train_data.splits[i])\n",
        "    test_data.set_index_map([test_data.splits[j] for j in range(0, i+1)])\n",
        "    \n",
        "    train_indices, val_indices = train_data.train_val_split(VAL_SIZE, RANDOM_SEED[run])\n",
        "    \n",
        "    train_subset[i] = copy(Subset(train_data, train_indices))\n",
        "    val_subset = Subset(train_data, val_indices)\n",
        "\n",
        "    tmp_dl = DataLoader(val_subset,\n",
        "                       batch_size=BATCH_SIZE,\n",
        "                       shuffle=True, \n",
        "                       num_workers=4,\n",
        "                       drop_last=True)\n",
        "    val_dataloader[i] = copy(tmp_dl)\n",
        "\n",
        "    tmp_dl = DataLoader(test_data,\n",
        "                       batch_size=BATCH_SIZE,\n",
        "                       shuffle=True, \n",
        "                       num_workers=4,\n",
        "                       drop_last=True)\n",
        "    test_dataloader[i] = copy(tmp_dl)\n",
        "\n",
        "  return train_dataloader, val_dataloader, test_dataloader, train_subset"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20APZIgI1eBy"
      },
      "source": [
        "### Going on with the model\n",
        "This is the main iCaRL step.\n",
        "\n",
        "This step is run 3 times with different `RANDOM_SEED`.\n",
        "Here the model is instantiated, trained and tested.\n",
        "\n",
        "Results and some statistics are then stored in the variable `logs`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJHKbipz0bMr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acc0f563-296d-4cd5-cb6e-22970876a3e0"
      },
      "source": [
        "logs = [[] for i in range(len(RANDOM_SEED))]\n",
        "best_net_tot_classes = [None for i in range(len(RANDOM_SEED))]\n",
        "\n",
        "for run in range(len(RANDOM_SEED)):\n",
        "  print(\"#################################\")\n",
        "  print(f\"Radom seed: {RANDOM_SEED[run]}\")\n",
        "  print(\"\")\n",
        "\n",
        "  # get data_subsets separated in incremental groups of 10 classes\n",
        "  train_dl, val_dl, test_dl, train_set = do_group_classes(run)\n",
        "\n",
        "  #create the resnet\n",
        "  net = resnet32()\n",
        "  \n",
        "  trainer = owrCosine(DEVICE,\n",
        "                  net,\n",
        "                  LR,\n",
        "                  MOMENTUM,\n",
        "                  WEIGHT_DECAY,\n",
        "                  MILESTONES,\n",
        "                  GAMMA,\n",
        "                  train_dl,\n",
        "                  val_dl,\n",
        "                  test_dl,\n",
        "                  BATCH_SIZE,\n",
        "                  train_set,\n",
        "                  train_transform,\n",
        "                  test_transform,\n",
        "                  TESTING_MODE,\n",
        "                  REJECTION_THRESHOLD) \n",
        "\n",
        "  #train and evaluate the model\n",
        "  logs[run] = trainer.train_model(NUM_EPOCHS)\n",
        "\n",
        "  best_net_tot_classes[run] = deepcopy(trainer.best_net)\n",
        "\n",
        "  print(\"#################################\")\n",
        "  print(\"\")\n",
        "  print(\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#################################\n",
            "Radom seed: 20\n",
            "\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Length of exemplars set: 0\n",
            "Epoch 1/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.24\n",
            "Best model updated\n",
            "\n",
            "Epoch 2/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.30\n",
            "Best model updated\n",
            "\n",
            "Epoch 3/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.43\n",
            "Best model updated\n",
            "\n",
            "Epoch 4/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.43\n",
            "Best model updated\n",
            "\n",
            "Epoch 5/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.52\n",
            "Best model updated\n",
            "\n",
            "Epoch 6/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.46\n",
            "\n",
            "Epoch 7/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.57\n",
            "Best model updated\n",
            "\n",
            "Epoch 8/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.57\n",
            "Best model updated\n",
            "\n",
            "Epoch 9/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.57\n",
            "Best model updated\n",
            "\n",
            "Epoch 10/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.55\n",
            "\n",
            "Epoch 11/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.60\n",
            "Best model updated\n",
            "\n",
            "Epoch 12/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.62\n",
            "Best model updated\n",
            "\n",
            "Epoch 13/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.58\n",
            "\n",
            "Epoch 14/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.65\n",
            "Best model updated\n",
            "\n",
            "Epoch 15/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.63\n",
            "\n",
            "Epoch 16/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.67\n",
            "Best model updated\n",
            "\n",
            "Epoch 17/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.65\n",
            "\n",
            "Epoch 18/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.63\n",
            "\n",
            "Epoch 19/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.70\n",
            "Best model updated\n",
            "\n",
            "Epoch 20/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.72\n",
            "Best model updated\n",
            "\n",
            "Epoch 21/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.71\n",
            "\n",
            "Epoch 22/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.71\n",
            "\n",
            "Epoch 23/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.69\n",
            "\n",
            "Epoch 24/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.73\n",
            "Best model updated\n",
            "\n",
            "Epoch 25/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.72\n",
            "\n",
            "Epoch 26/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.75\n",
            "Best model updated\n",
            "\n",
            "Epoch 27/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.74\n",
            "\n",
            "Epoch 28/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.78\n",
            "Best model updated\n",
            "\n",
            "Epoch 29/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.78\n",
            "\n",
            "Epoch 30/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.70\n",
            "\n",
            "Epoch 31/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.79\n",
            "Best model updated\n",
            "\n",
            "Epoch 32/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.81\n",
            "Best model updated\n",
            "\n",
            "Epoch 33/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.73\n",
            "\n",
            "Epoch 34/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.79\n",
            "\n",
            "Epoch 35/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.73\n",
            "\n",
            "Epoch 36/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.82\n",
            "Best model updated\n",
            "\n",
            "Epoch 37/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.84\n",
            "Best model updated\n",
            "\n",
            "Epoch 38/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.76\n",
            "\n",
            "Epoch 39/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.78\n",
            "\n",
            "Epoch 40/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.76\n",
            "\n",
            "Epoch 41/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.83\n",
            "\n",
            "Epoch 42/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.83\n",
            "\n",
            "Epoch 43/70 LR: [0.1]\n",
            "Validation accuracy on group 1/5: 0.83\n",
            "\n",
            "Epoch 44/70 LR: [0.020000000000000004]\n",
            "Validation accuracy on group 1/5: 0.85\n",
            "Best model updated\n",
            "\n",
            "Epoch 45/70 LR: [0.020000000000000004]\n",
            "Validation accuracy on group 1/5: 0.87\n",
            "Best model updated\n",
            "\n",
            "Epoch 46/70 LR: [0.020000000000000004]\n",
            "Validation accuracy on group 1/5: 0.87\n",
            "\n",
            "Epoch 47/70 LR: [0.020000000000000004]\n",
            "Validation accuracy on group 1/5: 0.85\n",
            "\n",
            "Epoch 48/70 LR: [0.020000000000000004]\n",
            "Validation accuracy on group 1/5: 0.87\n",
            "Best model updated\n",
            "\n",
            "Epoch 49/70 LR: [0.020000000000000004]\n",
            "Validation accuracy on group 1/5: 0.86\n",
            "\n",
            "Epoch 50/70 LR: [0.020000000000000004]\n",
            "Validation accuracy on group 1/5: 0.86\n",
            "\n",
            "Epoch 51/70 LR: [0.020000000000000004]\n",
            "Validation accuracy on group 1/5: 0.87\n",
            "\n",
            "Epoch 52/70 LR: [0.020000000000000004]\n",
            "Validation accuracy on group 1/5: 0.87\n",
            "\n",
            "Epoch 53/70 LR: [0.020000000000000004]\n",
            "Validation accuracy on group 1/5: 0.89\n",
            "Best model updated\n",
            "\n",
            "Epoch 54/70 LR: [0.020000000000000004]\n",
            "Validation accuracy on group 1/5: 0.87\n",
            "\n",
            "Epoch 55/70 LR: [0.020000000000000004]\n",
            "Validation accuracy on group 1/5: 0.88\n",
            "\n",
            "Epoch 56/70 LR: [0.020000000000000004]\n",
            "Validation accuracy on group 1/5: 0.87\n",
            "\n",
            "Epoch 57/70 LR: [0.020000000000000004]\n",
            "Validation accuracy on group 1/5: 0.87\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIUvaKyhx0rC"
      },
      "source": [
        "### Store logs in more usable dtype"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-uV6X0yxzQc"
      },
      "source": [
        "train_loss = [[logs[run_i]['group_train_loss'][i] for i in range(5)] for run_i in range(len(RANDOM_SEED))]\n",
        "train_accuracy = [[logs[run_i]['group_train_accuracies'][i] for i in range(5)] for run_i in range(len(RANDOM_SEED))]\n",
        "val_loss = [[logs[run_i]['val_losses'][i] for i in range(5)] for run_i in range(len(RANDOM_SEED))]\n",
        "val_accuracy = [[logs[run_i]['val_accuracies'][i] for i in range(5)] for run_i in range(len(RANDOM_SEED))]\n",
        "test_accuracy = [[logs[run_i]['test_accuracies'][i] for i in range(5)] for run_i in range(len(RANDOM_SEED))]\n",
        "predictions = [logs[run_i]['predictions'].cpu().data.numpy().tolist() for run_i in range(len(RANDOM_SEED))]\n",
        "true_labels = [logs[run_i]['true_labels'].cpu().data.numpy().tolist() for run_i in range(len(RANDOM_SEED))]\n",
        "\n",
        "open_values = [[logs[run_i]['open_values'][i] for i in range(5)] for run_i in range(len(RANDOM_SEED))]\n",
        "closed_values = [[logs[run_i]['closed_values'][i] for i in range(5)] for run_i in range(len(RANDOM_SEED))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r_ybcCLmc2R"
      },
      "source": [
        "### Save the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIG3hIOkyFNs"
      },
      "source": [
        "#### Saving logs in JSON files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mF_GR4KAyNYs"
      },
      "source": [
        "import json\n",
        "\n",
        "with open('states/095_train_loss.json', 'w') as f:\n",
        "  json.dump(train_loss, f)\n",
        "f.close\n",
        "with open('states/095_train_accuracy.json', 'w') as f:\n",
        "  json.dump(train_accuracy, f)\n",
        "f.close\n",
        "with open('states/095_val_loss.json', 'w') as f:\n",
        "  json.dump(val_loss, f)\n",
        "f.close  \n",
        "with open('states/095_val_accuracy.json', 'w') as f:\n",
        "  json.dump(val_accuracy, f)\n",
        "f.close\n",
        "with open('states/095_test_accuracy.json', 'w') as f:\n",
        "  json.dump(test_accuracy, f)\n",
        "f.close\n",
        "with open('states/095_predictions.json', 'w') as f:\n",
        "  json.dump(predictions, f)\n",
        "f.close\n",
        "with open('states/095_true_labels.json', 'w') as f:\n",
        "  json.dump(true_labels, f)\n",
        "f.close"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoVU9_V4zXOL"
      },
      "source": [
        "#### Saving best resnet on 100 classes for each seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tE-zisGejrST"
      },
      "source": [
        "for i in range(len(RANDOM_SEED)):\n",
        "  torch.save(best_net_tot_classes[i].state_dict(), \"states/095_bestnet_seed[{}]\".format(RANDOM_SEED[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NMLqiIS19IJ"
      },
      "source": [
        "### Print some graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6Mg2QiIBNRo"
      },
      "source": [
        "from data.utils_plot import plot_train_val, plot_test_accuracies, plot_confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2QzOlExmhKo"
      },
      "source": [
        "train_loss = np.array(train_loss)\n",
        "train_accuracy = np.array(train_accuracy)\n",
        "val_loss = np.array(val_loss)\n",
        "val_accuracy = np.array(val_accuracy)\n",
        "test_accuracy = np.array(test_accuracy)\n",
        "\n",
        "\n",
        "\n",
        "train_loss_stats = np.array([train_loss.mean(0), train_loss.std(0)]).transpose()\n",
        "train_accuracy_stats = np.array([train_accuracy.mean(0), train_accuracy.std(0)]).transpose()\n",
        "val_loss_stats = np.array([val_loss.mean(0), val_loss.std(0)]).transpose()\n",
        "val_accuracy_stats = np.array([val_accuracy.mean(0), val_accuracy.std(0)]).transpose()\n",
        "test_accuracy_stats = np.array([test_accuracy.mean(0), test_accuracy.std(0)]).transpose()\n",
        "\n",
        "\n",
        "open_values = np.array(open_values)\n",
        "closed_values = np.array(closed_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhDfQb32kuJg"
      },
      "source": [
        "threshold = 0.95\n",
        "print(f\"Threshold: {threshold}\")\n",
        "fig, axs = plt.subplots(1,2)\n",
        "fig.set_size_inches(15, 5)\n",
        "\n",
        "\n",
        "\n",
        "a,b,c = axs[0].hist(closed_values[0][4].cpu().numpy(), color = 'red', edgecolor = 'black', bins = [0,threshold,1])\n",
        "below = c[0].get_height()\n",
        "above = c[1].get_height()\n",
        "print(f\"closed_below:{below}, close_above:{above}\")\n",
        "print(f\"closed world rejection rate: {below/(below+above):.2f} (should be 0)\")\n",
        "\n",
        "\n",
        "a,b,c = axs[1].hist(open_values[0][4].cpu().numpy(), color = 'blue', edgecolor = 'black', bins = [0,threshold,1])\n",
        "below = c[0].get_height()\n",
        "above = c[1].get_height()\n",
        "print(f\"open_below:{below}, open_above:{above}\")\n",
        "print(f\"open world rejection rate: {below/(below+above):.2f} (should be 1)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rB0p1PlFxrc"
      },
      "source": [
        "#### Train validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAW6ze3-S_Kk"
      },
      "source": [
        "def plot_losses(train_stats, val_stats, loss = bool, save_directory = None):\n",
        "  train_mean = np.array(train_stats)[:, 0]\n",
        "  train_std = np.array(train_stats)[:, 1]\n",
        "  val_mean = np.array(val_stats)[:, 0]\n",
        "  val_std = np.array(val_stats)[:, 1]\n",
        "  fig, ax = plt.subplots(figsize = (10, 5), dpi = 100)\n",
        "  x = np.arange(10,51,10)\n",
        "  ax.errorbar(x, train_mean, train_std, label = 'Training')\n",
        "  ax.errorbar(x, val_mean, val_std, label = 'Validation')\n",
        "  if loss:\n",
        "    ax.set_title(\"Training and validation loss\")\n",
        "    ax.set_ylabel(\"Loss\")\n",
        "  else:\n",
        "    ax.set_title(\"Training and validation accuracy\")\n",
        "    ax.set_ylabel(\"Accuracy\")\n",
        "  ax.set_xlabel(\"Number of classes\")\n",
        "  plt.tight_layout()\n",
        "  ax.legend()\n",
        "  if save_directory != None:\n",
        "    fig.savefig(save_directory)\n",
        "  plt.show()\n",
        "\n",
        "plot_losses(train_loss_stats, val_loss_stats, loss = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb_RNTNKF18b"
      },
      "source": [
        "#### Train validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05IT5LFUBBBV"
      },
      "source": [
        "def plot_acc(train_stats, val_stats, loss = bool, save_directory = None):\n",
        "  train_mean = np.array(train_stats)[:, 0]\n",
        "  train_std = np.array(train_stats)[:, 1]\n",
        "  val_mean = np.array(val_stats)[:, 0]\n",
        "  val_std = np.array(val_stats)[:, 1]\n",
        "  fig, ax = plt.subplots(figsize = (10, 5), dpi = 100)\n",
        "  x = np.arange(10, 51, 10)\n",
        "  ax.errorbar(x, train_mean, train_std, label = 'Training')\n",
        "  ax.errorbar(x, val_mean, val_std, label = 'Validation')\n",
        "  if loss:\n",
        "    ax.set_title(\"Training and validation loss\")\n",
        "    ax.set_ylabel(\"Loss\")\n",
        "  else:\n",
        "    ax.set_title(\"Training and validation accuracy\")\n",
        "    ax.set_ylabel(\"Accuracy\")\n",
        "  ax.set_xlabel(\"Number of classes\")\n",
        "  plt.tight_layout()\n",
        "  ax.legend()\n",
        "  if save_directory != None:\n",
        "    fig.savefig(save_directory)\n",
        "  plt.show()\n",
        "\n",
        "plot_acc(train_accuracy_stats, val_accuracy_stats, loss = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llzylw_LF7e_"
      },
      "source": [
        "#### Test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bou4H0k9mteX"
      },
      "source": [
        "def plot_test(stats, save_directory = None):\n",
        "  mean = np.array(stats)[:, 0]\n",
        "  std = np.array(stats)[:, 1]\n",
        "  fig, ax = plt.subplots(figsize = (10, 5), dpi = 100)\n",
        "  x = np.arange(10, 51, 10)\n",
        "  ax.errorbar(x, mean, std)\n",
        "  ax.set_title(\"Test accuracy\")\n",
        "  ax.set_xlabel(\"Number of classes\")\n",
        "  ax.set_ylabel(\"Accuracy\")\n",
        "  plt.tight_layout()\n",
        "  if save_directory != None:\n",
        "    fig.savefig(save_directory)\n",
        "  plt.show()\n",
        "  \n",
        "plot_test(test_accuracy_stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gN6cighYm1nG"
      },
      "source": [
        "#### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPG6aNRjm5Se"
      },
      "source": [
        "for run in range(len(RANDOM_SEED)):\n",
        "  targets = np.array(true_labels[run])\n",
        "  preds = np.array(predictions[run])\n",
        "\n",
        "  plot_confusion_matrix(targets, preds, RANDOM_SEED[run], 't=0.95')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}